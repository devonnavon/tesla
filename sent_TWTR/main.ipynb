{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pd.set_option('expand_frame_repr', True)\n",
    "# pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet') #lexical db for english language that helps script determine base word \n",
    "# nltk.download('averaged_perceptron_tagger') #determines the context of a word in a sentence \n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "import re, string, random\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9953333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2069.8 : 1.0\n",
      "                      :) = True           Positi : Negati =    985.6 : 1.0\n",
      "                follower = True           Positi : Negati =     34.4 : 1.0\n",
      "                 welcome = True           Positi : Negati =     31.0 : 1.0\n",
      "                  arrive = True           Positi : Negati =     30.4 : 1.0\n",
      "                     bam = True           Positi : Negati =     24.3 : 1.0\n",
      "                     sad = True           Negati : Positi =     23.8 : 1.0\n",
      "                followed = True           Negati : Positi =     23.0 : 1.0\n",
      "                   enjoy = True           Positi : Negati =     21.0 : 1.0\n",
      "                     x15 = True           Negati : Positi =     18.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Interface\"\"\"\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':(', 4585),\n",
       " (':-(', 501),\n",
       " (\"i'm\", 343),\n",
       " ('...', 332),\n",
       " ('get', 325),\n",
       " ('miss', 291),\n",
       " ('go', 275),\n",
       " ('please', 275),\n",
       " ('want', 246),\n",
       " ('like', 218)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#freq distribution test\n",
    "all_pos_words = get_all_words(negative_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_pos.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['much', 'appreciate', 'mayor', 'fine']\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "#model test\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "etweet = elonTwts['cleaned'][2]\n",
    "etoken = remove_noise(etweet)\n",
    "\n",
    "print(etweet)\n",
    "print(classifier.classify(dict([token, True] for token in etweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twint\n",
    "\n",
    "# c = twint.Config()\n",
    "# c.Username = \"elonmusk\"\n",
    "# c.Search = \"tesla\"\n",
    "# c.Limit = 2000\n",
    "# c.Store_csv = True\n",
    "# c.Output = \"elonmusk_tesla.csv\"\n",
    "\n",
    "\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44196397</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>00:23:20</td>\n",
       "      <td>228</td>\n",
       "      <td>118</td>\n",
       "      <td>4119</td>\n",
       "      <td>Exactly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44196397</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>16:17:07</td>\n",
       "      <td>2457</td>\n",
       "      <td>5205</td>\n",
       "      <td>31262</td>\n",
       "      <td>Iâ€™m not messing around. Absurd &amp; medically irrational behavior in violation of constitutional civil liberties, moreover by *unelected* county offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44196397</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>15:43:26</td>\n",
       "      <td>196</td>\n",
       "      <td>155</td>\n",
       "      <td>5505</td>\n",
       "      <td>Much appreciated, Mayor Fine!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44196397</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>10:18:14</td>\n",
       "      <td>608</td>\n",
       "      <td>932</td>\n",
       "      <td>10070</td>\n",
       "      <td>Tesla is the biggest manufacturer in California &amp; second biggest exporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44196397</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>10:14:28</td>\n",
       "      <td>359</td>\n",
       "      <td>537</td>\n",
       "      <td>4599</td>\n",
       "      <td>Exactly! Tesla knows far more about what needs to be done to be safe through our Tesla China factory experience than an (unelected) interim junior...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  username        date      time  replies_count  retweets_count  \\\n",
       "0  44196397  elonmusk  2020-05-10  00:23:20            228             118   \n",
       "1  44196397  elonmusk  2020-05-09  16:17:07           2457            5205   \n",
       "2  44196397  elonmusk  2020-05-09  15:43:26            196             155   \n",
       "3  44196397  elonmusk  2020-05-09  10:18:14            608             932   \n",
       "4  44196397  elonmusk  2020-05-09  10:14:28            359             537   \n",
       "\n",
       "   likes_count  \\\n",
       "0         4119   \n",
       "1        31262   \n",
       "2         5505   \n",
       "3        10070   \n",
       "4         4599   \n",
       "\n",
       "                                                                                                                                                   tweet  \n",
       "0                                                                                                                                                Exactly  \n",
       "1  Iâ€™m not messing around. Absurd & medically irrational behavior in violation of constitutional civil liberties, moreover by *unelected* county offi...  \n",
       "2                                                                                                                          Much appreciated, Mayor Fine!  \n",
       "3                                                                              Tesla is the biggest manufacturer in California & second biggest exporter  \n",
       "4  Exactly! Tesla knows far more about what needs to be done to be safe through our Tesla China factory experience than an (unelected) interim junior...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "elonTwts = pd.read_csv(\"elonmusk_tesla.csv\")\n",
    "nullClms = elonTwts.columns[elonTwts.isna().any()].tolist() #null columns in list \n",
    "#elonTwts.columns.values\n",
    "elonTwts = elonTwts.drop(nullClms,axis=1) #dropping null columns \n",
    "#elonTwts.info()\n",
    "\n",
    "columnsMain = ['user_id','username','date','time','name','tweet','photos','timezone',\n",
    "               'mentions','replies_count','retweets_count','likes_count','reply_to']\n",
    "columnsTest = ['user_id','username','date','time','replies_count',\n",
    "               'retweets_count','likes_count','tweet']\n",
    "\n",
    "elonTwts = elonTwts[columnsTest]\n",
    "elonTwts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "['tokenized'] = [word_tokenize(tweet) for tweet in elonTwts['tweet']]\n",
    "# for tweet in elonTwts['tweet']:\n",
    "#     tokenized.append(word_tokenize(tweet))\n",
    "elonTwts['tokenized'] = tokenized\n",
    "\n",
    "cleaned = []\n",
    "for tweet in elonTwts['tokenized']:\n",
    "    cleaned.append(remove_noise(tweet, stop_words))\n",
    "elonTwts['cleaned'] = cleaned\n",
    "\n",
    "sentiment = []\n",
    "for tweet in elonTwts['cleaned']:\n",
    "    sentiment.append(classifier.classify(dict([token, True] for token in tweet)))\n",
    "elonTwts['sentiment'] = sentiment    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exactly</td>\n",
       "      <td>[Exactly]</td>\n",
       "      <td>[exactly]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iâ€™m not messing around. Absurd &amp; medically irrational behavior in violation of constitutional civil liberties, moreover by *unelected* county offi...</td>\n",
       "      <td>[I, â€™, m, not, messing, around, ., Absurd, &amp;, medically, irrational, behavior, in, violation, of, constitutional, civil, liberties, ,, moreover, b...</td>\n",
       "      <td>[â€™, mess, around, absurd, medically, irrational, behavior, violation, constitutional, civil, liberty, moreover, *unelected*, county, official, acc...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Much appreciated, Mayor Fine!</td>\n",
       "      <td>[Much, appreciated, ,, Mayor, Fine, !]</td>\n",
       "      <td>[much, appreciate, mayor, fine]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla is the biggest manufacturer in California &amp; second biggest exporter</td>\n",
       "      <td>[Tesla, is, the, biggest, manufacturer, in, California, &amp;, second, biggest, exporter]</td>\n",
       "      <td>[tesla, big, manufacturer, california, second, big, exporter]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exactly! Tesla knows far more about what needs to be done to be safe through our Tesla China factory experience than an (unelected) interim junior...</td>\n",
       "      <td>[Exactly, !, Tesla, knows, far, more, about, what, needs, to, be, done, to, be, safe, through, our, Tesla, China, factory, experience, than, an, (...</td>\n",
       "      <td>[exactly, tesla, know, far, need, safe, tesla, china, factory, experience, unelected, interim, junior, official, alameda, county]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   tweet  \\\n",
       "0                                                                                                                                                Exactly   \n",
       "1  Iâ€™m not messing around. Absurd & medically irrational behavior in violation of constitutional civil liberties, moreover by *unelected* county offi...   \n",
       "2                                                                                                                          Much appreciated, Mayor Fine!   \n",
       "3                                                                              Tesla is the biggest manufacturer in California & second biggest exporter   \n",
       "4  Exactly! Tesla knows far more about what needs to be done to be safe through our Tesla China factory experience than an (unelected) interim junior...   \n",
       "\n",
       "                                                                                                                                               tokenized  \\\n",
       "0                                                                                                                                              [Exactly]   \n",
       "1  [I, â€™, m, not, messing, around, ., Absurd, &, medically, irrational, behavior, in, violation, of, constitutional, civil, liberties, ,, moreover, b...   \n",
       "2                                                                                                                 [Much, appreciated, ,, Mayor, Fine, !]   \n",
       "3                                                                  [Tesla, is, the, biggest, manufacturer, in, California, &, second, biggest, exporter]   \n",
       "4  [Exactly, !, Tesla, knows, far, more, about, what, needs, to, be, done, to, be, safe, through, our, Tesla, China, factory, experience, than, an, (...   \n",
       "\n",
       "                                                                                                                                                 cleaned  \\\n",
       "0                                                                                                                                              [exactly]   \n",
       "1  [â€™, mess, around, absurd, medically, irrational, behavior, violation, constitutional, civil, liberty, moreover, *unelected*, county, official, acc...   \n",
       "2                                                                                                                        [much, appreciate, mayor, fine]   \n",
       "3                                                                                          [tesla, big, manufacturer, california, second, big, exporter]   \n",
       "4                      [exactly, tesla, know, far, need, safe, tesla, china, factory, experience, unelected, interim, junior, official, alameda, county]   \n",
       "\n",
       "  sentiment  \n",
       "0  Positive  \n",
       "1  Negative  \n",
       "2  Positive  \n",
       "3  Positive  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elonTwts[['tweet','tokenized','cleaned','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elonTwts[['tweet','tokenized','cleaned']].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    801\n",
       "Negative    598\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elonTwts['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tesla', 904),\n",
       " ('â€™', 313),\n",
       " ('http', 217),\n",
       " ('â€¦', 207),\n",
       " ('car', 206),\n",
       " ('model', 106),\n",
       " ('year', 98),\n",
       " ('good', 89),\n",
       " ('work', 83),\n",
       " ('make', 82)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elonWords = get_all_words(elonTwts.cleaned)\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_E = FreqDist(elonWords)\n",
    "freq_dist_E.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = elonTwts['cleaned'][1396]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(dict([token, True] for token in sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json') \n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#normalization\n",
    "from nltk.tag import pos_tag #https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_cleaned_tokens_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bf68c5d43ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_pos_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_cleaned_tokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_cleaned_tokens_list' is not defined"
     ]
    }
   ],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':)', 3691),\n",
       " (':-)', 701),\n",
       " (':d', 658),\n",
       " ('thanks', 388),\n",
       " ('follow', 357),\n",
       " ('love', 333),\n",
       " ('...', 290),\n",
       " ('good', 283),\n",
       " ('get', 263),\n",
       " ('thank', 253)]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_pos.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:8000]\n",
    "test_data = dataset[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9975\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2369.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1126.5 : 1.0\n",
      "                follower = True           Positi : Negati =     24.0 : 1.0\n",
      "                     bam = True           Positi : Negati =     22.2 : 1.0\n",
      "                     x15 = True           Negati : Positi =     21.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     21.2 : 1.0\n",
      "               community = True           Positi : Negati =     15.5 : 1.0\n",
      "                    blog = True           Positi : Negati =     14.2 : 1.0\n",
      "                    glad = True           Positi : Negati =     14.1 : 1.0\n",
      "                 welcome = True           Positi : Negati =     14.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "custom_tweet = \"\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "Accuracy is: 0.9963333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2061.9 : 1.0\n",
      "                      :) = True           Positi : Negati =   1649.7 : 1.0\n",
      "                     sad = True           Negati : Positi =     57.6 : 1.0\n",
      "                follower = True           Positi : Negati =     21.2 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     18.5 : 1.0\n",
      "               community = True           Positi : Negati =     15.5 : 1.0\n",
      "                 welcome = True           Positi : Negati =     13.7 : 1.0\n",
      "                    glad = True           Positi : Negati =     12.9 : 1.0\n",
      "                 perfect = True           Positi : Negati =     12.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "\n",
    "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "    \n",
    "   # print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2061.9 : 1.0\n",
      "                      :) = True           Positi : Negati =   1649.7 : 1.0\n",
      "                     sad = True           Negati : Positi =     57.6 : 1.0\n",
      "                follower = True           Positi : Negati =     21.2 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     18.5 : 1.0\n",
      "               community = True           Positi : Negati =     15.5 : 1.0\n",
      "                 welcome = True           Positi : Negati =     13.7 : 1.0\n",
      "                    glad = True           Positi : Negati =     12.9 : 1.0\n",
      "                 perfect = True           Positi : Negati =     12.2 : 1.0\n",
      "                     ugh = True           Negati : Positi =     11.1 : 1.0\n",
      "               goodnight = True           Positi : Negati =     10.9 : 1.0\n",
      "                     bro = True           Positi : Negati =     10.9 : 1.0\n",
      "                  friday = True           Positi : Negati =     10.7 : 1.0\n",
      "                followed = True           Negati : Positi =     10.5 : 1.0\n",
      "              appreciate = True           Positi : Negati =     10.2 : 1.0\n",
      "                    miss = True           Negati : Positi =      9.8 : 1.0\n",
      "                  hungry = True           Negati : Positi =      9.8 : 1.0\n",
      "           unfortunately = True           Negati : Positi =      9.8 : 1.0\n",
      "                   didnt = True           Negati : Positi =      9.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
